{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, Recurrent, LSTM, LSTMCell, Embedding, Dense\n",
    "from keras.layers import Bidirectional, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_context_vec( context_mat , att_weigts ):\n",
    "    att_weigts_rep = K.expand_dims( att_weigts , 2 )\n",
    "    att_weigts_rep = K.repeat_elements(att_weigts_rep , context_mat.shape[2] , 2 )\n",
    "    return K.sum(att_weigts_rep*context_mat , axis=1)\n",
    "\n",
    "\n",
    "def attend( key_vec , context_mat , contextMatTimeSteps , w1 , w2 ):\n",
    "    key_rep = K.repeat(key_vec , contextMatTimeSteps )\n",
    "    concated = K.concatenate([key_rep , context_mat ] , axis=-1 )\n",
    "    concated_r = K.reshape(concated , (-1 ,concated.shape[-1] ))\n",
    "    att_energies = K.dot( ( K.dot( concated_r , w1  )) , w2 )\n",
    "    att_energies = K.relu( K.reshape(att_energies  , (-1 , contextMatTimeSteps ) ) )\n",
    "    att_weigts = K.softmax( att_energies )\n",
    "    \n",
    "    return get_context_vec(context_mat ,att_weigts  ) , att_weigts\n",
    "    \n",
    "\n",
    "\n",
    "# the input is the  [ input , context_matrix ] \n",
    "\n",
    "class AttentionDecoder(Layer):\n",
    "\n",
    "    def __init__(self, rnn_cell  , **kwargs):\n",
    "        \n",
    "        self.output_dim = rnn_cell.state_size[0]\n",
    "        self.rnn_cell = rnn_cell\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert type( input_shape ) is list\n",
    "        assert len(input_shape) == 2 \n",
    "                \n",
    "\n",
    "        self.att_kernel = self.add_weight(name='att_kernel_1', \n",
    "                                      shape=( self.output_dim+input_shape[1][2] ,  input_shape[1][2] ),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "        self.att_kernel_2 = self.add_weight(name='att_kernel_2', \n",
    "                                      shape=( input_shape[1][2] ,  1 ),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "                \n",
    "        step_input_shape = (input_shape[0][0], input_shape[0][2]+input_shape[1][2] ) # batch_size , in_dim + contextVecDim \n",
    "        self.rnn_cell.build(step_input_shape)\n",
    "        \n",
    "        self._trainable_weights += ( self.rnn_cell.trainable_weights )\n",
    "        self._non_trainable_weights += (  self.rnn_cell.non_trainable_weights )\n",
    "        \n",
    "        self.contextMatTimeSteps = input_shape[1][1]\n",
    "                \n",
    "            \n",
    "        super(AttentionDecoder, self).build(input_shape)  \n",
    "            \n",
    "    \n",
    "    def get_initial_state(self, inputs):\n",
    "   \n",
    "        initial_state = K.zeros_like(inputs)   \n",
    "        initial_state = K.sum(initial_state, axis=(1, 2))   \n",
    "        initial_state = K.expand_dims(initial_state)   \n",
    "        if hasattr(self.rnn_cell.state_size, '__len__'):\n",
    "            return [K.tile(initial_state, [1, dim])  for dim in self.rnn_cell.state_size]\n",
    "        else:\n",
    "            return [K.tile(initial_state, [1, self.rnn_cell.state_size])]\n",
    "\n",
    "\n",
    "    def call(self, input ):\n",
    "        inputs , context_mat = input\n",
    "        \n",
    "        \n",
    "        def step(inputs, states):\n",
    "                hid = states[0]\n",
    "                ctx_vec , att_weigts = attend( hid , context_mat, self.contextMatTimeSteps , self.att_kernel , self.att_kernel_2 )\n",
    "                rnn_inp = K.concatenate( (inputs , ctx_vec ), axis=1 )\n",
    "                return self.rnn_cell.call( rnn_inp , states )\n",
    "            \n",
    "        timesteps = inputs.shape[ 1 ]\n",
    "        \n",
    "        initial_state = self.get_initial_state(inputs )\n",
    "        \n",
    "        last_output, outputs, states = K.rnn(step,\n",
    "                                             inputs,\n",
    "                                             initial_state,\n",
    "                                             input_length=timesteps)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape ):\n",
    "        return (input_shape[0][0], input_shape[0][1] , self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vatex_baseline_model():\n",
    "    \n",
    "    # Define an input sequence and process it.\n",
    "    encoder_inputs = Input(shape=(NUM_FEATURE, DIM_FEATURE))\n",
    "    encoder_states = Bidirectional(LSTM(DIM_HIDDEN, return_sequences=True))(encoder_inputs)\n",
    "    \n",
    "    decoder_inputs = Input((LEN_DEC_SEQ,  ))\n",
    "    decoder_outputs = Embedding(SIZE_VOCAB , 150 )( decoder_inputs )\n",
    "    \n",
    "    decoded_attn = AttentionDecoder(  LSTMCell(256) )([ decoder_outputs, encoder_states])\n",
    "    decoded = TimeDistributed( Dense(SIZE_VOCAB , activation='softmax') )( decoded_attn )\n",
    "    \n",
    "    model = Model( [encoder_inputs , decoder_inputs ] , decoded )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 100, 200)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 150)     1500000     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 100, 600)     1202400     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_decoder_2 (AttentionD (None, 300, 256)     1545368     embedding_2[0][0]                \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 300, 10000)   2570000     attention_decoder_2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,817,768\n",
      "Trainable params: 6,817,768\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURE = 100 # L\n",
    "DIM_FEATURE = 200\n",
    "DIM_HIDDEN = 300\n",
    "LEN_DEC_SEQ = 300\n",
    "SIZE_VOCAB = 10000\n",
    "\n",
    "model = vatex_baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: tokenize the data and store the tokenizer.\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "t.texts_to_sequences(docs)\n",
    "\n",
    "with open(\"tokenizer.pkl\", \"r\") as f:\n",
    "    pickle.dump(t, f)\n",
    "\n",
    "word2index = t.word_index\n",
    "index2word = dict(map(reversed, t.word_index.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do : load the I3D and preprocess the text.\n",
    "\n",
    "for ... in os.walk ...\n",
    "x_all = np.load(\"...\")\n",
    "y_all = ..\n",
    "\n",
    "def shuffle_split_data(X, y):\n",
    "    split = np.random.rand(X.shape[0]) < 0.7\n",
    "\n",
    "    X_Train = X[split]\n",
    "    y_Train = y[split]\n",
    "    X_Test =  X[~split]\n",
    "    y_Test = y[~split]\n",
    "\n",
    "    print len(X_Train), len(y_Train), len(X_Test), len(y_Test)\n",
    "    return X_Train, y_Train, X_Test, y_Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when train the model, endcode_inputs are just I3D data.\n",
    "# decoder_inputs, decoded are same. they are tokenized clip description\n",
    "# prediction are a little bit tricky. to visualize the result we also need the vocab in tokenizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, v_inputs, LEN_DEC_SEQ): \n",
    "    \n",
    "    m_input = [ v_inputs , np.zeros((1, LEN_DEC_SEQ)) ] \n",
    "    \n",
    "    res = []\n",
    "    for w_i in range(1, LEN_DEC_SEQ): \n",
    "        out = model.predict( m_input ) \n",
    "        out_w_i = out[0][w_i-1].argmax()  \n",
    "        if out_w_i == 0: \n",
    "            break\n",
    "        res.append(out_w_i)\n",
    "        m_input[1][0,w_i] = out_w_i \n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "congyuml",
   "language": "python",
   "name": "congyuml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
